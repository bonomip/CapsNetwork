{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73928d00",
   "metadata": {},
   "source": [
    "### Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49a542b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#mandatory for correct load and save of files\n",
    "%cd /Users/paolobonomi/work/python/capsnet\n",
    "\n",
    "# for project class\n",
    "import sys\n",
    "sys.path.append(\"./src\")\n",
    "\n",
    "from setup import Setup # set up model and dataset\n",
    "import perfu # performance function such as confusion matrix etc...\n",
    "import printer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d895f3",
   "metadata": {},
   "source": [
    "### AFFNIST Retrieve model and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bea27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_data(x, y, shuffle):\n",
    "    x_ = x\n",
    "    y_ = y\n",
    "\n",
    "    x_ = x_ / 255.0\n",
    "    x_ = tf.cast(x_, dtype=tf.float32)\n",
    "    x_ = tf.expand_dims(x_, axis=-1)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_, y_))\n",
    "    if shuffle:\n",
    "\n",
    "        dataset = dataset.shuffle(buffer_size=len(dataset), reshuffle_each_iteration=True)\n",
    "\n",
    "    dataset = dataset.batch(batch_size=64)\n",
    "    return x_, y_, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd49517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as spio\n",
    "import numpy as np\n",
    "\n",
    "#test set is 10k per batch, trainning set is 50k per batch\n",
    "\n",
    "def loadmat(filename):\n",
    "    '''\n",
    "    this function should be called instead of direct spio.loadmat\n",
    "    as it cures the problem of not properly recovering python dictionaries\n",
    "    from mat files. It calls the function check keys to cure all entries\n",
    "    which are still mat-objects\n",
    "    '''\n",
    "    data = spio.loadmat(filename, struct_as_record=False, squeeze_me=True)\n",
    "    return _check_keys(data)\n",
    "\n",
    "\n",
    "def _check_keys(dict):\n",
    "    '''\n",
    "    checks if entries in dictionary are mat-objects. If yes\n",
    "    todict is called to change them to nested dictionaries\n",
    "    '''\n",
    "    for key in dict:\n",
    "        if isinstance(dict[key], spio.matlab.mio5_params.mat_struct):\n",
    "            dict[key] = _todict(dict[key])\n",
    "    return dict        \n",
    "\n",
    "\n",
    "def _todict(matobj):\n",
    "    '''\n",
    "    A recursive function which constructs from matobjects nested dictionaries\n",
    "    '''\n",
    "    dict = {}\n",
    "    for strg in matobj._fieldnames:\n",
    "        elem = matobj.__dict__[strg]\n",
    "        if isinstance(elem, spio.matlab.mio5_params.mat_struct):\n",
    "            dict[strg] = _todict(elem)\n",
    "        else:\n",
    "            dict[strg] = elem\n",
    "    return dict\n",
    "\n",
    "def _unpack(dataset):\n",
    "    ans_set = dataset['affNISTdata']['label_int']\n",
    "    img_set = dataset['affNISTdata']['image']\n",
    "    trans = dataset['affNISTdata']['human_readable_transform']\n",
    "    \n",
    "    img_set = np.transpose(img_set)\n",
    "    img_set = np.reshape(img_set, (-1, 40, 40))\n",
    "    ans_set = ans_set.astype(np.uint8)\n",
    "\n",
    "    return img_set, ans_set, trans\n",
    "\n",
    "def load(train):\n",
    "    s = \"training\" if train else \"test\"\n",
    "\n",
    "    if train:\n",
    "        path = './data/affNIST/'+s+'_batches/1.mat'\n",
    "        data = _unpack(loadmat(path))\n",
    "    else:\n",
    "        path = './data/affNIST/'+s+'_batches/1.mat'\n",
    "        img_set, ans_set, trans = _unpack(loadmat(path))\n",
    "\n",
    "    return img_set, ans_set, trans\n",
    "\n",
    "def load_affnist_transformations(train):\n",
    "    s = \"training\" if train else \"test\"\n",
    "\n",
    "    if train:\n",
    "        path = './data/affNIST/'+s+'_batches/1.mat'\n",
    "        data = _unpack(loadmat(path))\n",
    "    else:\n",
    "        path = './data/affNIST/'+s+'_batches/1.mat'\n",
    "        img_set, ans_set, trans = _unpack(loadmat(path))\n",
    "\n",
    "    return trans\n",
    "\n",
    "def load_MNIST(train):\n",
    "    s = \"training\" if train else \"test\"\n",
    "    path = './data/affNIST/originals/'+s+'.mat'\n",
    "    img_set, ans_set, trans = _unpack(loadmat(path))\n",
    "    return img_set, ans_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d813e17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as fn\n",
    "\n",
    "def convert_to_torch(img):\n",
    "    rx = torch.from_numpy(img)\n",
    "    rx = torch.unsqueeze(rx, 0)\n",
    "    return rx\n",
    "\n",
    "def convert_to_tensor(img):\n",
    "    rx = tf.convert_to_tensor(img, dtype=tf.dtypes.float32)\n",
    "    rx = np.reshape(rx, [1, 40, 40])\n",
    "    return rx\n",
    "\n",
    "def apply_scaling(img, x, y, mode):\n",
    "    i = int(40*y)\n",
    "    j = int(40*x)\n",
    "    rx = fn.resize(img, size=[ i, j ], interpolation=mode)\n",
    "    rx = fn.center_crop(rx, [40, 40])\n",
    "    return rx\n",
    "\n",
    "def apply_transformation(img, data, id, mode=transforms.InterpolationMode.BILINEAR):\n",
    "    rotation = data[0][id] #counter clock wise between -20 and +20\n",
    "    shearing = data[1][id] #shearing between -0.2 and +0.2. if shear 1 horiz line turns into 45 degree line\n",
    "    y_scale = data[2][id]\n",
    "    x_scale = data[3][id] # between 0.8 (shrinking by 20%) and 1.2 (making 20% larger).\n",
    "    v_trasl = data[4][id]\n",
    "    h_trasl = data[5][id] # \n",
    "    \n",
    "    rx = convert_to_torch(img)    \n",
    "    rx = fn.affine(rx, rotation*-1, [0, 0], 1, [0, shearing*-45], interpolation=mode)\n",
    "    rx = apply_scaling(rx, x_scale, y_scale, mode)\n",
    "    #rx = fn.affine(rx, 0, [h_trasl*0.4, v_trasl*0.4], 1, 0, interpolation=mode)\n",
    "\n",
    "    rx = convert_to_tensor(rx)\n",
    "    rx = tf.reshape(rx, [40, 40])\n",
    "    return rx\n",
    "\n",
    "def create_affnist(train):\n",
    "    x, y = load_MNIST(train)\n",
    "    t = load_affnist_transformations(train)\n",
    "\n",
    "    for i in range(0, x.shape[0]): \n",
    "        x[i] = apply_transformation(x[i], t, i)\n",
    "        \n",
    "    return x, y\n",
    "\n",
    "def check_bounds(x, rx):\n",
    "    a = np.sum(x)\n",
    "    b = torch.sum(rx).item()\n",
    "    c = abs(a - b)\n",
    "    return c >= 1\n",
    "\n",
    "def apply_random_translation(img, size=40, mode=transforms.InterpolationMode.BILINEAR):\n",
    "    rx = convert_to_torch(img)\n",
    "    \n",
    "    flag = True;\n",
    "    while flag:\n",
    "        _rx = rx\n",
    "        _rx = transforms.RandomAffine(0, translate=(0.6,0.6), interpolation=mode)(rx)\n",
    "        flag = check_bounds(img, _rx)\n",
    "        \n",
    "    rx = convert_to_tensor(_rx)\n",
    "    rx = tf.reshape(rx, [size, size])\n",
    "    return rx\n",
    "\n",
    "\n",
    "def create_random_mnist(train):\n",
    "    x, y = load_MNIST(train)\n",
    "    \n",
    "    for i in range(0, x.shape[0]):\n",
    "        x[i] = apply_random_translation(x[i])\n",
    "    \n",
    "    return x, y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8268a1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = load_MNIST(False)\n",
    "aff = load(False)\n",
    "my_aff = create_affnist(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7ab74e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for id in range(0, 10):\n",
    "    original_image = mnist[0][id]\n",
    "    affnist_image = aff[0][id]\n",
    "    my_image = my_aff[0][id]\n",
    "\n",
    "    printer.print_image(original_image, mnist[1][id], 40)\n",
    "    printer.print_image(my_image, my_aff[1][id], 40)\n",
    "    printer.print_image(affnist_image, aff[1][id], 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49460f99",
   "metadata": {},
   "source": [
    "# TEST IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e069c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Setup.GEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02c5397",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_id = Setup.GEN[0]\n",
    "model_version = \"_3e-5\"\n",
    "dataset_version = \"_v1\"\n",
    "learning_rate = 3e-5\n",
    "setup = Setup()\n",
    "\n",
    "#load original mnist dataset and model\n",
    "x_t, y_t, b_t = setup.load_data(Setup.GEN[0], train=True, version=dataset_version, create=False)\n",
    "model = setup.init_model(model_id, model_version, x_t, y_t, learning_rate, \"v2\")\n",
    "model3 = setup.init_model(model_id, model_version, x_t, y_t, learning_rate, \"v3\")\n",
    "\n",
    "#load affnist\n",
    "x, y, b = setup.load_data(Setup.GEN[1], train=False, version=dataset_version, create=False)\n",
    "\n",
    "#load mnist\n",
    "x_2, y_2, b_2 = setup.load_data(Setup.GEN[0], train=False, version=dataset_version, create=False)\n",
    "\n",
    "#recreate affnist\n",
    "m_x, m_y = create_affnist(False)\n",
    "m_x, m_y, m_b = _process_data(m_x, m_y, False) \n",
    "\n",
    "setup.load_ckpt(model, 1)\n",
    "setup.load_ckpt(model3, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42aee893",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db018d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ccea13",
   "metadata": {},
   "source": [
    "# Model v2 accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2e1a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#aff_acc = setup.get_accuracy(model, b, setup.get_total_images(x))\n",
    "#print(aff_acc)\n",
    "#  0.2143"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93f3dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mnist_acc = setup.get_accuracy(model, b_2, setup.get_total_images(x_2))\n",
    "#print(mnist_acc)\n",
    "#  0.9915"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd06647c",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_aff_acc = setup.get_accuracy(model, m_b, setup.get_total_images(m_x))\n",
    "print(my_aff_acc)\n",
    "\n",
    "# interpolation mode BILINEAR = 0.2114\n",
    "# interpolation mode NEAREST = 0.2127\n",
    "# interpolation mode NEAREST = 0.2113 - no shearing\n",
    "# interpolation mode NEAREST = 0.2189 - no scaling\n",
    "# interpolation mode NEAREST = 0.9559 - no transaltion\n",
    "# interpolation mode BILINEAR = 0.9613 - no transaltion\n",
    "# interpolation mode NEARESR = 0.8766 - traslation only at 0.2 percent then original\n",
    "# interpolation mode NEARESR = 0.6259 - traslation only at 0.4 percent then original"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3515d70",
   "metadata": {},
   "source": [
    "# Augmentation on mnist (random pos) 40x40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce66be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_random_mnist(train):\n",
    "    s = 'train' if train else 'test'\n",
    "\n",
    "    print(\"Load Custom rMNIST \"+s+\" dataset v1... \")\n",
    "\n",
    "    X_ = np.load('./data/rMNIST/x_'+s+'_v1.npy')\n",
    "    y_ = np.load('./data/rMNIST/y_'+s+'_v1.npy')\n",
    "    \n",
    "    return X_, y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2803e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_x, rm_y = load_random_mnist(False)\n",
    "rm_x, rm_y, rm_b = _process_data(rm_x, rm_y, False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f66d203",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for id in range(0, 10):\n",
    "    original_image = mnist[0][id]\n",
    "    my_image = rm_x[id]\n",
    "\n",
    "    printer.print_image(original_image, mnist[1][id], 40)\n",
    "    printer.print_image(my_image, my_aff[1][id], 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccbe683",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "665744eb",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001326ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def get_confusion_mat(model, batch, name):\n",
    "    matrix = np.zeros((10,10))\n",
    "    tot_imgs = 0\n",
    "    \n",
    "    with tqdm(total=len(batch)) as pbar:\n",
    "        \n",
    "        description = \"Creating \"+name+\" confusion matrix\"\n",
    "        pbar.set_description_str(description)\n",
    "        for X_batch, y_batch in batch:\n",
    "            \n",
    "            tot_imgs = tot_imgs + len(X_batch)\n",
    "            \n",
    "            res = model.predict(X_batch)\n",
    "            for i in range(0, y_batch.shape[0]):\n",
    "                \n",
    "                matrix[y_batch[i], res[i]] += 1 # row, col\n",
    "        \n",
    "            pbar.update(1)\n",
    "\n",
    "    return matrix, tot_imgs\n",
    "\n",
    "def print_matrix(matrix, matrix_type, model_type, data_type, no_images, col_labels=0, row_labels=0, size_x=10, size_y=10, color=\"Blues\"):\n",
    "        headers = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
    "        \n",
    "        title = model_type+\" confusion matrix \"+matrix_type+\",\\n on \"+str(no_images/1000)+\"K \"+data_type+\" images\"\n",
    "        x_label = \"Actual Digit\"\n",
    "        y_label = \"Predicted Digit\"\n",
    "        \n",
    "        if row_labels != 0:\n",
    "            fig, ax = plt.subplots(figsize=(size_x,size_y))\n",
    "            mat = matrix.copy()\n",
    "            mat = mat.transpose()\n",
    "            \n",
    "            ax = sns.heatmap(mat, annot=True, fmt='g', cmap=color, ax=ax)\n",
    "\n",
    "            ax.set_title(title+'\\n\\n', size=size_x*1.9)\n",
    "            ax.set_xlabel('\\n'+y_label, size=size_x*1.5)\n",
    "            ax.set_ylabel(x_label+' \\n', size=size_x*1.5)\n",
    "            \n",
    "            ax3 = ax.twiny()\n",
    "            # ax3.set_aspect(\"equal\")\n",
    "            ax3.set_xlabel(\"No. of occurencies\\n\", size=size_x*1.5)\n",
    "            ax3.set_xlim([0,ax.get_xlim()[1]])\n",
    "            ax3.set_xticks(ax.get_xticks())\n",
    "            ax3.set_xticklabels(row_labels, fontsize=size_x*1)\n",
    "            ax3.tick_params(top=False)\n",
    "            ax3.spines['top'].set_visible(False)\n",
    "            ax3.spines['right'].set_visible(False)\n",
    "            ax3.spines['bottom'].set_visible(False)\n",
    "            ax3.spines['left'].set_visible(False)\n",
    "        else:\n",
    "            fig, ax = plt.subplots(figsize=(size_x,size_y))\n",
    "            ax = sns.heatmap(matrix, annot=True, fmt='g', cmap=color, ax=ax)\n",
    "\n",
    "            ax.set_title(title+'\\n\\n', size=size_x*1.9)\n",
    "            ax.set_xlabel('\\n'+x_label, size=size_x*1.5)\n",
    "            ax.set_ylabel(y_label+' \\n', size=size_x*1.5)\n",
    "\n",
    "\n",
    "        if col_labels != 0:            \n",
    "            ax3 = ax.twiny()\n",
    "            # ax3.set_aspect(\"equal\")\n",
    "            ax3.set_xlabel(\"No. of occurencies\\n\", size=size_x*1.5)\n",
    "            ax3.set_xlim([0,ax.get_xlim()[1]])\n",
    "            ax3.set_xticks(ax.get_xticks())\n",
    "            ax3.set_xticklabels(col_labels, fontsize=size_x*1)\n",
    "            ax3.tick_params(top=False)\n",
    "            ax3.spines['top'].set_visible(False)\n",
    "            ax3.spines['right'].set_visible(False)\n",
    "            ax3.spines['bottom'].set_visible(False)\n",
    "            ax3.spines['left'].set_visible(False)  \n",
    "\n",
    "        ## Ticket labels - List must be in alphabetical order\n",
    "        ax.xaxis.set_ticklabels(headers)\n",
    "        ax.yaxis.set_ticklabels(headers)\n",
    "\n",
    "        ## Display the visualization of the Confusion Matrix.\n",
    "        plt.show()\n",
    "        \n",
    "def normalize_matrix(matrix, tot):\n",
    "    mat = matrix.copy()\n",
    "    for i in range(0, 10):\n",
    "        for j in range(0, 10):\n",
    "            mat[i, j] = (mat[i, j] / tot) * 100  \n",
    "    return  np.around(mat, decimals=3)\n",
    "\n",
    "def normalize_matrix_on_row(matrix):\n",
    "    mat = matrix.copy()\n",
    "    for c in range(0,10):\n",
    "        \n",
    "        mat[:,c] /= np.sum(mat[:,c])\n",
    "        mat[:,c] *= 100 \n",
    "                           \n",
    "    return np.around(mat, decimals=3)\n",
    "\n",
    "def normalize_matrix_on_columns(matrix):\n",
    "    mat = matrix.copy()\n",
    "    for r in range(0,10):\n",
    "        \n",
    "        mat[r,:] /= np.sum(mat[r,:])\n",
    "        mat[r,:] *= 100\n",
    "                           \n",
    "    return np.around(mat, decimals=3)\n",
    "\n",
    "def get_sum_row_matrix_label(matrix):\n",
    "    res = []\n",
    "    for c in range(0, 10):\n",
    "        res.append( str( int( np.sum( matrix[:,c] ) ) ) )\n",
    "    return res\n",
    "\n",
    "def get_sum_colum_matrix_label(matrix):\n",
    "    res = []\n",
    "    for r in range(0, 10):\n",
    "        res.append( str( int( np.sum( matrix[r,:] ) ) ) )\n",
    "    return res\n",
    "\n",
    "#matrix\n",
    "#n = tot images : number\n",
    "#v = model description : string\n",
    "#dt = dataset type description : string\n",
    "def print_all_matrix(mat, n, v, dt):\n",
    "    print_matrix( normalize_matrix(mat, n),\n",
    "                            'normalized on total images(%)', \n",
    "                            v, \n",
    "                            '{0} test'.format(dt),  \n",
    "                             n)\n",
    "\n",
    "    print_matrix( normalize_matrix_on_row(mat),\n",
    "                            'normalized on on total images in row(%)', \n",
    "                            v, \n",
    "                            '{0} test'.format(dt),  \n",
    "                             n,\n",
    "                             row_labels=get_sum_row_matrix_label(mat))\n",
    "\n",
    "    print_matrix( normalize_matrix_on_columns(mat),\n",
    "                            'normalized on total images in column(%)', \n",
    "                            v, \n",
    "                            '{0} test'.format(dt),  \n",
    "                             n,\n",
    "                             col_labels=get_sum_colum_matrix_label(mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c65ca6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mnist matrix\n",
    "#matrix, tot_images = get_confusion_mat(model, b_2, \"MNIST/MNIST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449f4448",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_version = \"Model v2\"\n",
    "dataset_type = \"MNIST\"\n",
    "print_all_matrix(matrix, tot_images, model_version, dataset_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca51af9",
   "metadata": {},
   "source": [
    "# Confusion table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84116e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import sqrt \n",
    "import pandas as pd\n",
    "\n",
    "def _get_confusion_table_for(mat, digit, tot):\n",
    "    \n",
    "    dict_ = {}\n",
    "    \n",
    "    # condition positive (how many images were < digit >)\n",
    "    p = 0\n",
    "    for i in range(0, 10):\n",
    "        p += mat[i, digit]\n",
    "    dict_[\"Condition Positive\"] = p\n",
    "    \n",
    "    #condition negative (how many images were not < digit > )\n",
    "    n = tot - p\n",
    "    dict_[\"Condition Negative\"] = n \n",
    "    \n",
    "    tp = mat[digit, digit]\n",
    "    dict_[\"True Positive (TN)\"] = tp\n",
    "    \n",
    "    fn = -tp\n",
    "    for i in range(0, 10):\n",
    "        fn += mat[i, digit]\n",
    "    dict_[\"False Negative (FN)\"] = fn\n",
    "    \n",
    "    fp = -tp\n",
    "    for i in range(0, 10):\n",
    "        fp += mat[digit, i]\n",
    "    dict_[\"False Positive (FP)\"] = fp \n",
    "    \n",
    "    tn = tot + tp    \n",
    "    for i in range(0, 10):\n",
    "        tn -= mat[digit, i] + mat[i, digit]\n",
    "    dict_[\"True Negative (TN)\"] = tn \n",
    "\n",
    "    tpr = tp / p\n",
    "    dict_[\"Sensitivity (TPR)\"] = tpr \n",
    "\n",
    "    tnr = tn / n\n",
    "    dict_[\"Specificity (TNR)\"] = tnr \n",
    "\n",
    "    ppv = tp / (tp + fp)\n",
    "    dict_[\"Positive Predictive Value (PPV)\"] = ppv\n",
    "\n",
    "    npv = tn / ( tn + fn )\n",
    "    dict_[\"Negative Predictive Value (NPV)\"] = npv \n",
    "\n",
    "    fnr = 1 - tpr\n",
    "    dict_[\"False Negative Rate (FNR)\"] = fnr \n",
    "\n",
    "    fpr = 1 - tnr\n",
    "    dict_[\"False Positive Rate (FPR)\"] = fpr \n",
    "\n",
    "    fdr = 1 - ppv\n",
    "    dict_[\"False Discovery Rate (FDR)\"] = fdr \n",
    "\n",
    "    for_ = 1 - npv\n",
    "    dict_[\"False Omission Rate (FOR)\"] = for_ \n",
    "\n",
    "    lr_p = tpr/fpr\n",
    "    dict_[\"Positive Likelihood Ratio (LR+)\"] = int(lr_p) \n",
    "    \n",
    "    lr_n = fnr/tnr\n",
    "    dict_[\"Negative Likelihood Ratio (LR-)\"] = lr_n \n",
    "\n",
    "    pt = sqrt( fpr ) / ( sqrt(tpr) * sqrt(fpr) )\n",
    "    dict_[\"Prevalence Threshold (PT)\"] = pt \n",
    "\n",
    "    ts = tp / ( tp + fn + fp )\n",
    "    dict_[\"Threat Score\"] = ts \n",
    "\n",
    "    prevalence = p / tot\n",
    "    dict_[\"Prevalence\"] = prevalence \n",
    "    \n",
    "    acc = ( tp + tn ) / tot\n",
    "    dict_[\"Accuracy (ACC)\"] = acc \n",
    "\n",
    "    ba = ( tpr * tnr ) / 2\n",
    "    dict_[\"Balanced Accuracy\"] = ba \n",
    "\n",
    "    f1 = ( 2*tp ) / ( 2*tp + fp + fn )\n",
    "    dict_[\"F1 Score\"] = f1 \n",
    "\n",
    "    mcc = ( ( tp * tn ) - ( fp * fn ) ) / sqrt( ( tp+fp )*( tp*fn )*( tn+fp )*(tn+fn) )\n",
    "    dict_[\"Matthews Correlation Coefficient (MCC)\"] = mcc\n",
    "    \n",
    "    fm = sqrt( ppv * tpr )\n",
    "    dict_[\"Fowlkesâ€“Mallows Index (FM)\"] = fm\n",
    "    \n",
    "    bm = tpr + tnr - 1\n",
    "    dict_[\"Bookmaker Informedness (BM)\"] = bm\n",
    "    \n",
    "    mk = ppv + npv - 1\n",
    "    dict_[\"Markedness (MK)\"] = mk\n",
    "    \n",
    "    #from 0 to inf\n",
    "    dor = lr_p / lr_n\n",
    "    dict_[\"Diagnostic odds ratio (DOR)\"] = int(dor)\n",
    "    \n",
    "    header = np.array(list(dict_.keys()))\n",
    "    values = np.array(list(dict_.values()))\n",
    "    \n",
    "    return values, header\n",
    "\n",
    "def _trim(string):\n",
    "    if string.endswith(\".0\"):\n",
    "        return string[0:-2]\n",
    "    return string\n",
    "\n",
    "def _get_confusion_table(confusion_matrix, dataset_size):\n",
    "    values = []\n",
    "    index = []\n",
    "    columns = []\n",
    "    for i in range(0, 10):\n",
    "        v, h = _get_confusion_table_for(confusion_matrix, i, dataset_size)\n",
    "        values.append(v)\n",
    "        index = h\n",
    "        columns.append(i)\n",
    "    \n",
    "    values = np.around(np.array(values), decimals=3)\n",
    "    index = np.array(index)\n",
    "    columns = np.array(columns)\n",
    "\n",
    "    return values, columns, index\n",
    "\n",
    "def print_confusion_table(confusion_matrix, tot_images):\n",
    "    values, columns, index = _get_confusion_table(confusion_matrix, tot_images)\n",
    "    df = pd.DataFrame( values , index=columns, columns=index, dtype=str).T\n",
    "    df.columns.name = \"Digit\"\n",
    "    df = df.applymap(_trim)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294ad5ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = print_confusion_table(matrix, tot_images)\n",
    "#print(df.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc86cca1",
   "metadata": {},
   "source": [
    "### Error cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9b4729",
   "metadata": {},
   "outputs": [],
   "source": [
    "off = 44\n",
    "n = 110\n",
    "idx, pred = perfu.get_error_index(model, X_test[off:off+n], y_test[off:off+n], off)\n",
    "\n",
    "print(idx)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061c4de7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(idx)):\n",
    "    img = idx[i]\n",
    "    printer.print_image_and_prediction(X_test[img], y_test[img], pred[i], 40 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d25f921",
   "metadata": {},
   "source": [
    "### Network Conv1/PrimaryCapsule Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba7c1c4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "printer.print_network(model, X_test[45], y_test[45], 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c7d52f",
   "metadata": {},
   "source": [
    "### Network Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b206e3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "printer.print_fixed_network_params(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "27039add6037919ceee90f8f6a1ded5e18fb50dba46fc6f2fa7bb5396680f76d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
