{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80a44220-5073-4038-a3d6-b6fab0554beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "317d8b5b-387e-429d-bd39-ff56894fddbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prints(text, shape):\n",
    "    print(text + \" \" + str(shape))\n",
    "    \n",
    "def printw(w): # use it only in the first example\n",
    "    print(\"\\n first filter\\n\")\n",
    "    print(w[:, :, 0, 0]) # first filter first dimension\n",
    "    print()\n",
    "    print(w[:, :, 1, 0]) # first filter second dimension\n",
    "    print()\n",
    "    print(w[:, :, 2, 0]) # first filter third dimension\n",
    "    print(\"\\n second filter\\n\")\n",
    "    print(w[:, :, 0, 1]) # second filter first dimension\n",
    "    print()\n",
    "    print(w[:, :, 1, 1]) # second filter second dimension\n",
    "    print()\n",
    "    print(w[:, :, 2, 1]) # second filter third dimension\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1804e378-308c-4cd5-829b-26dd20ee18db",
   "metadata": {},
   "source": [
    "# CONVOLUTION ON MULTI CHANNEL IMAGE\n",
    "\n",
    "Applying a 2D Convolution (with kernel size of 9), our kernel will look different to the single channel case (layer Conv1). Given we have 256 input channels this time, our kernel will be initialised with 256 channels too. So even though we are using a 2D Convolution, we have a 3D kernel.\n",
    "\n",
    "A 2D Convolution just means we slide the kernel along two dimension, it doesn’t necessarily define the shape of the kernel, since that depends on the shape of the input channels too. Viewed like this, we think as if each channel has its own 9x9 kernel. A kernel still looks at patterns across channels though, since we have the cross channel summation at the end\n",
    "\n",
    "ref @ https://medium.com/apache-mxnet/multi-channel-convolutions-explained-with-ms-excel-9bbf8eb77108"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e5b954-0362-421b-baa3-bd45943b94f6",
   "metadata": {},
   "source": [
    "# CONVOLUTION 2D\n",
    "\n",
    "When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers or None, does not include the sample axis), e.g. input_shape=(128, 128, 3) for 128x128 RGB pictures in data_format=\"channels_last\". You can use None when a dimension has variable size.\n",
    "\n",
    "ref @ https://keras.io/api/layers/convolution_layers/convolution2d/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "216cd447-142e-4a50-bc1e-f0b7fa163243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(input_shape, trim, number_of_filters, kernel_size):\n",
    "    x = tf.random.normal(input_shape)\n",
    "    #use trim to select only an image inside the batch\n",
    "    layer = tf.keras.layers.Conv2D(number_of_filters, kernel_size, activation='relu', input_shape=input_shape[trim:])\n",
    "    y = layer(x)\n",
    "    prints(\"Weights shape:\", layer.get_weights()[0].shape)\n",
    "    prints(\"Biases shape:\", layer.get_weights()[1].shape)\n",
    "    print(\"Output shape:\", y.shape)\n",
    "    return layer.get_weights()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07213486-41b9-4c5f-b443-21c608358763",
   "metadata": {},
   "source": [
    "# 28x28 RGB with batch size 4\n",
    "a 3D Convolution with kernel shape `[kernel_size, kernel_size, input_shape_channels]` would be equivalent in this situation, but with a 2D Convolution you don’t need to specify the channel dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19d0c2f6-1da1-4199-ac77-47dea8648d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape_channels = 256\n",
    "input_shape = (1, 20, 20, input_shape_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41efd66e-bbc2-47f7-a6d8-fd829936a5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "Weights shape: (9, 9, 256, 1)\n",
      "Biases shape: (1,)\n",
      "Output shape: (1, 12, 12, 1)\n",
      "\n",
      " first filter\n",
      "\n",
      "[[-0.00344403  0.00281168 -0.01366857 -0.01588221  0.0165191   0.00189028\n",
      "   0.0111421  -0.00513314 -0.01263403]\n",
      " [-0.00287339 -0.0083472   0.00249976  0.00726387 -0.00120004 -0.01400942\n",
      "   0.01107622 -0.01068502 -0.00281196]\n",
      " [ 0.00484966  0.00063701  0.00714438 -0.01612107  0.00612163  0.00034693\n",
      "  -0.00808859  0.00086837  0.01240266]\n",
      " [-0.01056118  0.00233591  0.0162074  -0.01346876  0.00677721 -0.01288093\n",
      "  -0.00464178  0.00707137  0.01311765]\n",
      " [ 0.00307134 -0.00108475 -0.01227147  0.01169262  0.00816006 -0.00556105\n",
      "   0.01311235  0.01052781 -0.00377508]\n",
      " [ 0.00876319  0.00624191  0.01406711 -0.01150734 -0.01449971  0.00621694\n",
      "  -0.00563188  0.00659081 -0.00091576]\n",
      " [ 0.00450041  0.00364252  0.00619032 -0.00028204  0.00938283 -0.00250939\n",
      "   0.01500637 -0.01602203  0.0148125 ]\n",
      " [-0.00410395 -0.00198714  0.01060809  0.01287853 -0.01432481 -0.01683761\n",
      "  -0.01330769  0.01538709 -0.01684378]\n",
      " [-0.00440585 -0.01120174  0.00541728 -0.0064972  -0.00850364  0.01056342\n",
      "   0.00433469  0.00738957  0.00908022]]\n",
      "\n",
      "[[-6.92300126e-03 -8.01753160e-03 -1.35245081e-02  7.47546554e-04\n",
      "  -1.19645502e-02 -1.12374686e-03  8.01086426e-05 -1.50631741e-02\n",
      "   1.28280353e-02]\n",
      " [ 1.35685988e-02  1.77633017e-03 -1.06175747e-02 -6.60333596e-03\n",
      "   5.68730384e-03  4.17098962e-03  1.59180295e-02 -1.36584900e-02\n",
      "  -1.31737515e-02]\n",
      " [-1.67851485e-02 -1.08499760e-02 -8.66510905e-03  1.69115234e-02\n",
      "   4.66338545e-03 -1.29240826e-02  9.74705070e-03 -4.00674623e-03\n",
      "  -3.96822859e-03]\n",
      " [-1.22958329e-02 -1.04581807e-02 -2.51384266e-03  1.63079072e-02\n",
      "   8.75608437e-03 -1.67914983e-02  1.11738518e-02  2.48933211e-04\n",
      "   6.06699660e-03]\n",
      " [-6.40461501e-03 -1.29954517e-02 -1.32443393e-02  5.17608970e-03\n",
      "  -1.50283361e-02  4.00543213e-04  6.72099367e-03  1.35843325e-02\n",
      "   1.65099408e-02]\n",
      " [ 4.61735204e-03 -1.65496096e-02  7.10684806e-04  1.97693706e-03\n",
      "  -4.75226529e-03  1.60380881e-02 -5.15193678e-03 -4.94043343e-03\n",
      "  -1.31083736e-02]\n",
      " [ 1.23117566e-02 -1.20713804e-02 -1.16342874e-02 -8.39352142e-03\n",
      "  -8.64071306e-03  1.30676012e-02 -1.68341659e-02 -1.36916367e-02\n",
      "  -1.18685029e-02]\n",
      " [-7.70635437e-03 -9.81007703e-03  8.66786949e-03  5.34933805e-03\n",
      "  -2.16530077e-03  8.44728574e-03  2.73469836e-04  7.02946819e-03\n",
      "   6.99076429e-03]\n",
      " [ 1.16830133e-02  9.05884802e-03 -1.37899723e-03  2.49015167e-03\n",
      "   2.97708809e-03 -1.45117100e-02  1.02255885e-02 -4.52930201e-03\n",
      "  -1.65287312e-02]]\n",
      "\n",
      "[[ 1.31936781e-02 -1.45106129e-02 -1.28732352e-02 -9.24990140e-03\n",
      "   1.08310413e-02 -1.63782910e-02 -2.32176390e-03 -7.89992604e-03\n",
      "   1.39780026e-02]\n",
      " [-3.75707261e-03 -2.78103817e-03 -5.66215627e-03  1.58852991e-02\n",
      "  -1.68619603e-02 -8.46142136e-03  1.43443365e-02  1.16801001e-02\n",
      "  -1.25437006e-02]\n",
      " [ 5.52259572e-03  9.95847397e-03  8.63474235e-03  1.20142568e-02\n",
      "   1.08384807e-02 -5.75049687e-03 -6.47855364e-03 -6.30016811e-03\n",
      "   1.45639535e-02]\n",
      " [-7.90744741e-03 -1.02835223e-02  6.19575009e-03 -1.34822745e-02\n",
      "  -1.55401006e-02  4.33381647e-05  1.01486705e-02 -4.50927857e-03\n",
      "   2.46461108e-03]\n",
      " [-9.06337239e-03 -7.35549256e-03 -1.47752464e-02 -2.75700726e-03\n",
      "  -1.48188844e-02  4.61769104e-03 -1.25288982e-02  1.40741505e-02\n",
      "  -3.05498578e-03]\n",
      " [-7.73850922e-03 -1.00654289e-02  1.52308736e-02 -6.51212502e-03\n",
      "   3.77701968e-03  6.70640916e-03 -4.86541819e-03  3.92942317e-03\n",
      "  -8.38226452e-03]\n",
      " [-5.58530260e-03 -3.49113625e-03  1.56626459e-02 -3.37745994e-04\n",
      "   5.65210171e-03  1.50180422e-03 -4.52323072e-03 -1.13681033e-02\n",
      "   7.63567165e-04]\n",
      " [ 1.29128136e-02 -1.65132005e-02 -1.54995108e-02  7.15069659e-03\n",
      "  -7.01303408e-03  4.39635105e-03 -9.76148061e-03 -6.00764994e-03\n",
      "  -4.21654526e-03]\n",
      " [-1.29094496e-02 -1.37322508e-02  1.25983655e-02 -1.51198702e-02\n",
      "   1.47543903e-02  5.86426072e-03  1.44027639e-02 -6.97051734e-03\n",
      "   1.26584973e-02]]\n",
      "\n",
      " second filter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-01 09:21:18.611746: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-06-01 09:21:18.611816: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 3 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m kernel_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m9\u001b[39m \u001b[38;5;66;03m# it's se same of kenrel_size = [2, 2]\u001b[39;00m\n\u001b[1;32m      4\u001b[0m w \u001b[38;5;241m=\u001b[39m conv2d(input_shape, \u001b[38;5;241m1\u001b[39m, number_of_filters, kernel_size)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mprintw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mprintw\u001b[0;34m(w)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(w[:, :, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m]) \u001b[38;5;66;03m# first filter third dimension\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m second filter\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mw\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m) \u001b[38;5;66;03m# second filter first dimension\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(w[:, :, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m]) \u001b[38;5;66;03m# second filter second dimension\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 3 with size 1"
     ]
    }
   ],
   "source": [
    "number_of_filters = 1\n",
    "kernel_size = 9 # it's se same of kenrel_size = [2, 2]\n",
    "\n",
    "w = conv2d(input_shape, 1, number_of_filters, kernel_size)\n",
    "\n",
    "printw(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5bd695-518d-434d-9ea6-bf264837f95c",
   "metadata": {},
   "source": [
    "# 28x28 GRAYSCALE with batch size 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "626932ba-4a41-4d71-9573-4ec1d6187611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights shape: (2, 2, 1, 5)\n",
      "Biases shape: (5,)\n",
      "Output shape: (4, 27, 27, 5)\n"
     ]
    }
   ],
   "source": [
    "input_shape = (4, 28, 28, 1)\n",
    "\n",
    "number_of_filters = 5\n",
    "kernel_size = 2\n",
    "\n",
    "w = conv2d(input_shape, 1, number_of_filters, kernel_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea7a3f8-b988-45a4-81db-bc1671963ea1",
   "metadata": {},
   "source": [
    "# CONVOLUTION 3D\n",
    "\n",
    "When using this layer as the first layer in a model, provide the keyword argument\n",
    "input_shape (tuple of integers or None, does not include the sample axis), \n",
    "e.g. input_shape=(128, 128, 128, 1) for 128x128x128 volumes with a single channel,\n",
    "in data_format=\"channels_last\".\n",
    "\n",
    "ref @ https://keras.io/api/layers/convolution_layers/convolution3d/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a36c3134-eced-4913-9016-089bc426a6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3d(input_shape, trim, number_of_filters, kernel_size):\n",
    "    x = tf.random.normal(input_shape)\n",
    "    layer = tf.keras.layers.Conv3D(number_of_filters, kernel_size, activation='relu', input_shape=input_shape[trim:])\n",
    "    y = layer(x)\n",
    "    prints(\"Layer weights shape:\", layer.get_weights()[0].shape)\n",
    "    prints(\"Layer biases shape:\", layer.get_weights()[1].shape)\n",
    "    print(\"Layer Output shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a91dcd4-64c2-400f-b4a1-5eaa3cd947f5",
   "metadata": {},
   "source": [
    "# 28x28x28 volumes with a single channel and batch size 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f19700a-d09a-40db-8cf4-7873383aba5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape =(4, 28, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb28855f-498a-4f39-8c0d-752e3bc91a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer weights shape: (2, 2, 2, 1, 5)\n",
      "Layer biases shape: (5,)\n",
      "Layer Output shape: (4, 27, 27, 27, 5)\n"
     ]
    }
   ],
   "source": [
    "number_of_filters = 5\n",
    "kernel_size = 2 # it's se same of kenrel_size = [3, 3, 3]\n",
    "\n",
    "conv3d(input_shape, 1, number_of_filters, kernel_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fd7fb8-149a-41cd-85a9-eae61b492a3c",
   "metadata": {},
   "source": [
    "# 28x28x28 volumes with 3 channels\n",
    "e.g. a batch of 4 videos of 3D frames, with 7 frames per video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "585ff71d-0215-44d0-a956-7133c39cb400",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (4, 7, 28, 28, 28, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26ef3626-9606-46f8-9a09-4b164a1c2913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer weights shape: (2, 2, 2, 3, 5)\n",
      "Layer biases shape: (5,)\n",
      "Layer Output shape: (4, 7, 27, 27, 27, 5)\n"
     ]
    }
   ],
   "source": [
    "number_of_filters = 5\n",
    "kernel_size = 2\n",
    "\n",
    "conv3d(input_shape, 2, number_of_filters, kernel_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
